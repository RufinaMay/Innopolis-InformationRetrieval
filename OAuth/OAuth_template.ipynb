{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "OAuth_template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grmiKH5q7fTI",
        "colab_type": "text"
      },
      "source": [
        "# What do you store in your Google Drive?\n",
        "\n",
        "Sometimes it can be quite troublesome to crawl web data - for example, when you can't just collect data from web-pages because the authentification to a website is required. Today's tutorial is about a dataset of special type - namely, Google Drive data. You will need to get access to the system using OAuth protocol and download and parse files of different types.\n",
        "\n",
        "Plan. \n",
        "1. Download [this little archive](https://drive.google.com/open?id=1Xji4A_dEAm_ycnO0Eq6vxj7ThcqZyJZR), **unzip** it and place the folder anywhere inside your Google Drive. You should get a subtree of 6 folders with files of different types: presentations, pdf-files, texts, and even code.\n",
        "2. Go to [Google Drive API](https://developers.google.com/drive/api/v3/quickstart/python) documentation, read [intro](https://developers.google.com/drive/api/v3/about-sdk) and learn how to [search for files](https://developers.google.com/drive/api/v3/reference/files/list) and [download](https://developers.google.com/drive/api/v3/manage-downloads) them.\n",
        "3. Learn how to open from python such files as [pptx](https://python-pptx.readthedocs.io/en/latest/user/quickstart.html), pdf, docx or even use generalized libraries like [textract](https://textract.readthedocs.io/en/stable/index.html).\n",
        "4. Build search index (preferably, inverted one) based on the documents you get and learn to retrieve file names (e.g. `at least this file.txt`) in response to a query. Validate your code on the following set of queries (there are documents for each of them!):\n",
        "```\n",
        "segmentation\n",
        "algorithm\n",
        "classifer\n",
        "printf\n",
        "predecessor\n",
        "Шеннон\n",
        "Huffman\n",
        "function\n",
        "constructor\n",
        "machine learning\n",
        "dataset\n",
        "Протасов\n",
        "Protasov\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "damxFNwf7fTM",
        "colab_type": "text"
      },
      "source": [
        "## 2. Access GDrive ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdsWRLkI7fTN",
        "colab_type": "text"
      },
      "source": [
        "This is the example of how you can oranize your code - it's fine if you change it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBFpTT8D7fTO",
        "colab_type": "text"
      },
      "source": [
        "Let's extract the list of all files that are contained (recursively) in the folder of interest. In my case, I called it `air_oauth_folder`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw2ghljc7fTP",
        "colab_type": "code",
        "outputId": "e7749d6d-4dff-4bf6-c904-9b9d3038860d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: google-api-python-client in /usr/local/lib/python3.6/dist-packages (1.7.11)\n",
            "Requirement already up-to-date: google-auth-httplib2 in /usr/local/lib/python3.6/dist-packages (0.0.3)\n",
            "Requirement already up-to-date: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.11.3)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Rqbm_9KJ7fTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import pickle\n",
        "import os\n",
        "from googleapiclient.discovery import build\n",
        "from apiclient.http import MediaIoBaseDownload\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from google.auth.transport.requests import Request\n",
        "import io\n",
        "\n",
        "class Drive:\n",
        "    def __init__(self, ):\n",
        "        \"\"\"\n",
        "        init connection to google drive\n",
        "        \"\"\"\n",
        "        SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
        "        creds = None\n",
        "        if os.path.exists('token.pickle'):\n",
        "            with open('token.pickle', 'rb') as token:\n",
        "                creds = pickle.load(token)\n",
        "        # If there are no (valid) credentials available, let the user log in.\n",
        "        if not creds or not creds.valid:\n",
        "            if creds and creds.expired and creds.refresh_token:\n",
        "                creds.refresh(Request())\n",
        "            else:\n",
        "                flow = InstalledAppFlow.from_client_secrets_file(\n",
        "                    'credentials.json', SCOPES)\n",
        "                creds = flow.run_local_server(port=0)\n",
        "            # Save the credentials for the next run\n",
        "            with open('token.pickle', 'wb') as token:\n",
        "                pickle.dump(creds, token)\n",
        "        self.service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "    def gdrive_get_all_files_in_folder(self, folder_name):\n",
        "        results = self.service.files().list(q=f\"name = '{folder_name}'\",\n",
        "                                            fields='nextPageToken, files(id, name)').execute()\n",
        "        parents, result_files = [], [] # initial set of parents\n",
        "        for file in results.get('files', []):\n",
        "            parents.append(file)\n",
        "        # now while parents are not empty we keep on retrieving files that are children of them\n",
        "        while(len(parents)>0):\n",
        "            # take children of the first parent in the list and delete from list of parents that we have to check\n",
        "            parent = parents.pop()\n",
        "            results = self.service.files().list(q=f\"'{parent['id']}' in parents\",\n",
        "                                              fields='nextPageToken, files(id, name)').execute()\n",
        "            children = results.get('files', [])\n",
        "            if children:\n",
        "                for file in children:\n",
        "                    parents.append(file)\n",
        "            else:\n",
        "                result_files.append(parent)\n",
        "        return result_files\n",
        "\n",
        "    def gdrive_download_file(self, file, path_to_save): \n",
        "        if not os.path.exists(path_to_save):\n",
        "            os.makedirs(path_to_save)\n",
        "        #TODO download file and save it under the path\n",
        "        request = self.service.files().get_media(fileId=file['id'])\n",
        "#         fh = io.BytesIO()\n",
        "        fh = open(os.path.join(path_to_save, file['name']), 'wb')\n",
        "        fh_str = open(os.path.join(path_to_save, file['name']), 'w')\n",
        "        downloader = MediaIoBaseDownload(fh, request)\n",
        "        downloader_str = MediaIoBaseDownload(fh_str, request)\n",
        "        done = False\n",
        "        while done is False:\n",
        "          try: status, done = downloader.next_chunk()\n",
        "          except:\n",
        "            try: status, done = downloader_str.next_chunk()\n",
        "            except:\n",
        "              done = True\n",
        "              continue\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apOpgoh57fTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# folder_of_interest = 'air_oauth_folder'\n",
        "drive = Drive()\n",
        "folder_of_interest = 'data'\n",
        "files = drive.gdrive_get_all_files_in_folder(folder_of_interest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eJoQYeBx7fTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = \"test_files\"\n",
        "for item in files:\n",
        "    drive.gdrive_download_file(item, test_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq6r8hcq7fTe",
        "colab_type": "text"
      },
      "source": [
        "## 2. Tests ##\n",
        "Please fill free to change function signatures and behaviour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw5XIFFL7fTg",
        "colab_type": "code",
        "outputId": "9e551480-d06c-404c-d9ee-1b9ce33a2192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "assert len(files) == 34, 'Number of files is incorrect'\n",
        "print('n_files:', len(files))\n",
        "\n",
        "print(\"file here means id and name, e.g.: \", files[0])\n",
        "\n",
        "drive.gdrive_download_file(files[0], '.')\n",
        "\n",
        "import os.path\n",
        "assert os.path.isfile(os.path.join('.', files[0]['name'])), \"File is not downloaded correctly\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_files: 34\n",
            "file here means id and name, e.g.:  {'id': '1_Prdscwt_Pu2_Zb5yoJTEP-QZSacQlNy', 'name': 'bloomset.js'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJsLTrkL7fTk",
        "colab_type": "text"
      },
      "source": [
        "## 3. Read files ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFTqaO8_7fTl",
        "colab_type": "text"
      },
      "source": [
        "Write here the code to extract text from the files you just downoaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V318M0oF7fTn",
        "colab_type": "code",
        "outputId": "3fc58786-13e6-4aea-8a2c-351e55828633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install textract"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textract\n",
            "  Downloading https://files.pythonhosted.org/packages/32/31/ef9451e6e48a1a57e337c5f20d4ef58c1a13d91560d2574c738b1320bb8d/textract-1.6.3-py3-none-any.whl\n",
            "Collecting SpeechRecognition==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 115kB/s \n",
            "\u001b[?25hCollecting xlrd==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 28.8MB/s \n",
            "\u001b[?25hCollecting docx2txt==0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Collecting python-pptx==0.6.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/86/eb979f7b0333ec769041aae36df8b9f1bd8bea5bbad44620663890dce561/python-pptx-0.6.18.tar.gz (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 38.3MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.7MB/s \n",
            "\u001b[?25hCollecting extract-msg==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/90/84485a914ed90adb5e87df17e626be04162fbba146dfecf34643659a4633/extract_msg-0.23.1-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hCollecting EbookLib==0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/38/7d6ab2e569a9165249619d73b7bc6be0e713a899a3bc2513814b6598a84c/EbookLib-0.17.1.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 43.6MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20181108\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/6e8746e6965d1a7ea8e97253e3d79e625da5547e8f376f88de5d024bacb9/pdfminer.six-20181108-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from textract) (1.12.0)\n",
            "Collecting argcomplete==1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/82/f44c9661e479207348a979b1f6f063625d11dc4ca6256af053719bbb0124/argcomplete-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from textract) (3.0.4)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (6.2.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/1f/2092a81056d36c1b6651a645aa84c1f76bcee03103072d4fe1cb58501d69/XlsxWriter-1.2.8-py2.py3-none-any.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 51.6MB/s \n",
            "\u001b[?25hCollecting soupsieve>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/05/cf/ea245e52f55823f19992447b008bcbb7f78efc5960d77f6c34b5b45b36dd/soupsieve-2.0-py2.py3-none-any.whl\n",
            "Collecting olefile==0.46\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 51.0MB/s \n",
            "\u001b[?25hCollecting imapclient==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/39/e1c2c2c6e2356ab6ea81fcfc0a74b044b311d6a91a45300811d9a6077ef7/IMAPClient-2.1.0-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.6/dist-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/16/da16a22d47bac9bf9db39f3b9af74e8eeed8855c0df96be20b580ef92fff/pycryptodome-3.9.7-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20181108->textract) (2.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2018.9)\n",
            "Building wheels for collected packages: docx2txt, python-pptx, EbookLib, olefile\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp36-none-any.whl size=3963 sha256=a508e7d595022a74a7bee3cbcbc3177f88d4264e6083a432801db511d8f1214c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.18-cp36-none-any.whl size=275706 sha256=67a954d7e153f5cbd814cdcda8deef48d76804d937d566a8d9e27bd6380705c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/1f/2c/29acca422b420a0b5210bd2cd7e9669804520d602d2462f20b\n",
            "  Building wheel for EbookLib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EbookLib: filename=EbookLib-0.17.1-cp36-none-any.whl size=38164 sha256=aca73bb541f835e31c8ac842c565481ae183ef20cd51ddda82d613a6cda223c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/11/01/951369cbbf8f96878786a1f4da68bd7ac19a5d945b38e03d54\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=62bb686f516f0a367100ef09db1645dc554e56b39b188ba5b61e032f4de34196\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built docx2txt python-pptx EbookLib olefile\n",
            "Installing collected packages: SpeechRecognition, xlrd, docx2txt, XlsxWriter, python-pptx, soupsieve, beautifulsoup4, olefile, imapclient, extract-msg, EbookLib, pycryptodome, pdfminer.six, argcomplete, textract\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed EbookLib-0.17.1 SpeechRecognition-3.8.1 XlsxWriter-1.2.8 argcomplete-1.10.0 beautifulsoup4-4.8.0 docx2txt-0.8 extract-msg-0.23.1 imapclient-2.1.0 olefile-0.46 pdfminer.six-20181108 pycryptodome-3.9.7 python-pptx-0.6.18 soupsieve-2.0 textract-1.6.3 xlrd-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBn_nfyV1vgY",
        "colab_type": "code",
        "outputId": "929eb952-8037-451f-b672-998ae8174e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "!pip install python-docx \n",
        "!pip install tika"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=c30b692d7d7956f7a4124d73894d35386ada8f6720d1f958d85ae6cce7dd8570\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.10\n",
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/c3/088827903bc1862f67b185e1df428071b8da6118155c1b46bcb0c61992ea/tika-1.23.1.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (45.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.23.1-cp36-none-any.whl size=32561 sha256=10d04135f47678e9df98a09c06fa45bc8790bc35309d79c6f2889ecc058a13b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/6b/6d/c850c2a934057edce9779d41400d910c6a9b1f22027566b10f\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.23.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGSf7-ui7fTq",
        "colab_type": "code",
        "outputId": "ac813122-bee8-4778-d525-9cb1f2f12a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# for windows please refer to https://textract.readthedocs.io/en/latest/installation.html#don-t-see-your-operating-system-installation-instructions-here\n",
        "# https://www.xpdfreader.com/download.html\n",
        "# ALSO BE CAREFUL WITH SPACES IN NAMES. Better save without spaces!!!!!\n",
        "\n",
        "import textract \n",
        "import nltk\n",
        "from tika import parser\n",
        "from pptx import Presentation\n",
        "from docx import Document\n",
        "nltk.download('punkt')\n",
        "\n",
        "def extratc_docx(path):\n",
        "    doc = Document(path)\n",
        "    fullText = []\n",
        "    for para in doc.paragraphs:\n",
        "        fullText.append(para.text)\n",
        "    return ' '.join(fullText)\n",
        "\n",
        "def extract_pptx(path):\n",
        "  prs = Presentation(path)\n",
        "  text_runs = []\n",
        "\n",
        "  for slide in prs.slides:\n",
        "      for shape in slide.shapes:\n",
        "          if not shape.has_text_frame:\n",
        "              continue\n",
        "          for paragraph in shape.text_frame.paragraphs:\n",
        "              for run in paragraph.runs:\n",
        "                  text_runs.append(run.text)\n",
        "  return ' '.join(text_runs)\n",
        "\n",
        "def extract_pdf(path):\n",
        "  raw = parser.from_file(path)\n",
        "  return raw['content'].replace('\\n', ' ').strip()\n",
        "\n",
        "def get_file_strings(path):\n",
        "    #TODO change this function to handle different data types properly - textract is not able to parse everything\n",
        "    # Take care of non-text data too (and delete non text data?)\n",
        "    # take file content as I understand\n",
        "    # and we should skip mp3 and avi files as well as I get\n",
        "    if path.endswith('.mp3'):\n",
        "      return None\n",
        "    try: \n",
        "      texts = ''\n",
        "      f = open(path, 'r')\n",
        "      while True: \n",
        "        # Get next line from file \n",
        "        line = f.readline() \n",
        "        if not line: \n",
        "            break\n",
        "        texts += line.replace('\\n', ' ')\n",
        "      return texts\n",
        "    except:\n",
        "      pass\n",
        "    try: return extract_pptx(path)\n",
        "    except: \n",
        "      pass\n",
        "    try: return extratc_docx(path)\n",
        "    except: \n",
        "      pass\n",
        "    try: return extract_pdf(path)\n",
        "    except:\n",
        "      print(path)\n",
        "      return None"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gLentP-17fTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6fdffbc8-c7b5-4227-804f-fe63b1bf7c7f"
      },
      "source": [
        "# creating dictionary of parsed files\n",
        "files_data = dict()\n",
        "for file in os.scandir(test_dir): \n",
        "    strings = get_file_strings(file.path)\n",
        "    # print(strings)\n",
        "    if strings:\n",
        "        files_data[file.name] = strings"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-05 13:39:27,267 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.23/tika-server-1.23.jar to /tmp/tika-server.jar.\n",
            "2020-03-05 13:39:27,953 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.23/tika-server-1.23.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2020-03-05 13:39:28,370 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msgSMlFW7fTw",
        "colab_type": "text"
      },
      "source": [
        "## 3. Tests ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRIXTLnk7fTx",
        "colab_type": "code",
        "outputId": "562bf60a-da0b-4a10-cd35-042d5c837bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "assert len(files_data) == 31\n",
        "print(len(files_data))\n",
        "assert \"Protasov\" in get_file_strings(os.path.join(test_dir, 'at least this file.txt')), \"TXT File parsed incorrectly\"\n",
        "assert \"A. Image classification\" in get_file_strings(os.path.join(test_dir, 'deep-features-scene (1).pdf')), \"PDF File parsed incorrectly\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtRRLa2w7fT0",
        "colab_type": "text"
      },
      "source": [
        "## 4. Index and search ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrpC6NKX7fT1",
        "colab_type": "text"
      },
      "source": [
        "Build a search index based on files you just parsed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl8kdyL77fT2",
        "colab_type": "code",
        "outputId": "88cf8487-2cd2-4d8d-b6cf-0a40681e7012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class Preprocessor:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stop_words = nltk.corpus.stopwords.words('english')\n",
        "        self.ps = nltk.stem.PorterStemmer()\n",
        "\n",
        "\n",
        "    # word tokenize text using nltk lib\n",
        "    def tokenize(self, text):\n",
        "        return nltk.word_tokenize(text)\n",
        "\n",
        "\n",
        "    # stem word using provided stemmer\n",
        "    def stem(self, word, stemmer):\n",
        "        return stemmer.stem(word)\n",
        "\n",
        "\n",
        "    # check if word is appropriate - not a stop word and isalpha, \n",
        "    # i.e consists of letters, not punctuation, numbers, dates\n",
        "    def is_apt_word(self, word):\n",
        "        return word not in self.stop_words and word.isalpha()\n",
        "\n",
        "\n",
        "    # combines all previous methods together\n",
        "    # tokenizes lowercased text and stems it, ignoring not appropriate words\n",
        "    def preprocess(self, text):\n",
        "        tokenized = self.tokenize(text.lower())\n",
        "        return [self.stem(w, self.ps) for w in tokenized if self.is_apt_word(w)]\n",
        "\n",
        "def build_inverted_index(files_data):\n",
        "  index = dict()\n",
        "  # doc_names = dict()\n",
        "  def index_doc(doc_content, doc_id):\n",
        "    prep = Preprocessor()\n",
        "    doc_content = prep.preprocess(doc_content)\n",
        "    article_index = Counter(doc_content)\n",
        "    for term in article_index.keys():\n",
        "        article_freq = article_index[term]\n",
        "        if term not in index:                \n",
        "            index[term] = [article_freq, (doc_id, article_freq)]\n",
        "        else:\n",
        "            index[term][0] += article_freq\n",
        "            index[term].append((doc_id, article_freq))\n",
        "  #TODO build search index from files\n",
        "  for doc_id, file_name in enumerate(files_data):\n",
        "    # doc_names[doc_id] = file_name\n",
        "    index_doc(files_data[file_name], file_name)\n",
        "  return index"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSiR-BYv7fT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inverted_index = build_inverted_index(files_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APIiLtnu7fT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QueryProcessing:\n",
        "    @staticmethod\n",
        "    def prepare_query(raw_query):\n",
        "        prep = Preprocessor()\n",
        "        # pre-process query the same way as documents\n",
        "        query = prep.preprocess(raw_query)\n",
        "        # count frequency\n",
        "        return Counter(query)\n",
        "    \n",
        "    @staticmethod\n",
        "    def boolean_retrieval(query, index):\n",
        "        postings = []\n",
        "        for term in query.keys():\n",
        "            if term not in index:  # ignoring absent terms\n",
        "                continue\n",
        "            posting = index[term][1:]\n",
        "            # extract document info only\n",
        "            posting = [i[0] for i in posting]\n",
        "            postings.append(posting)\n",
        "        docs = set.intersection(*map(set,postings))\n",
        "        \n",
        "        return docs \n",
        "\n",
        "def find(query, index):\n",
        "    #TODO implement search procedure\n",
        "    # preprocess query\n",
        "    query = QueryProcessing.prepare_query(query)\n",
        "    return QueryProcessing.boolean_retrieval(query, index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izEZFS6f7fT_",
        "colab_type": "text"
      },
      "source": [
        "## 4. Tests ## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYdOR1px7fUA",
        "colab_type": "code",
        "outputId": "7b567c85-259f-4cab-cfbe-5c786fec4ace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "queries = [\"segmentation\", \"algorithm\", \"printf\", \"predecessor\", \"Huffman\",\n",
        "           \"function\", \"constructor\", \"machine learning\", \"dataset\", \"Protasov\"]\n",
        "\n",
        "for query in queries:\n",
        "    r = find(query, inverted_index)\n",
        "    print(\"Results for: \", query)\n",
        "    print(\"\\t\", r)\n",
        "    assert len(r) > 0, \"Query should return at least 1 document\"\n",
        "    assert len(r) > 1, \"Query should return at least 2 documents\"\n",
        "    assert \"at least this file.txt\" in r, \"This file has all the queries. It should be in a result\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for:  segmentation\n",
            "\t {'deep-features-scene (1).pdf', 'at least this file.txt'}\n",
            "Results for:  algorithm\n",
            "\t {'Tutorial 9.pdf', 'grant.txt', 'dsa.pdf', 'at least this file.txt', 'nn.cpp', 'grant-translate.txt', '[DM]-Course Description.docx', 'deep-features-scene (1).pdf', 'DSA_15 Lion in the desert.pptx', 'DSA_09 - 2-3-4 and B-Trees.pdf', 'Tutorial #8.pdf', 'cs.pdf', 'retake-2016-08-18.docx'}\n",
            "Results for:  printf\n",
            "\t {'lockexamples.c', 'rdtsc-gcc.c', 'cyclomat.c', 'at least this file.txt'}\n",
            "Results for:  predecessor\n",
            "\t {'skiplist.js', 'Tutorial 9.pdf', 'DSA_09 - 2-3-4 and B-Trees.pdf', 'at least this file.txt'}\n",
            "Results for:  Huffman\n",
            "\t {'DSA_15 Lion in the desert.pptx', 'dsa.pdf', 'at least this file.txt'}\n",
            "Results for:  function\n",
            "\t {'sort.js', 'dsa.pdf', 'at least this file.txt', 'bloomset.js', 'grant-translate.txt', '[DM]-Course Description.docx', 'neuro.html', 'deep-features-scene (1).pdf', 'DSA_15 Lion in the desert.pptx', 'FuncnNEW.pdf', 'Tutorial #8.pdf', 'Assessment Criteria (May).pdf', 'retake-2016-08-18.docx'}\n",
            "Results for:  constructor\n",
            "\t {'bloomset.js', 'Tutorial 9.pdf', 'skiplist.js', 'at least this file.txt'}\n",
            "Results for:  machine learning\n",
            "\t {'grant.txt', 'dsa.pdf', 'at least this file.txt', '[DM]-Course Description.docx', 'deep-features-scene (1).pdf', 'cs.pdf'}\n",
            "Results for:  dataset\n",
            "\t {'grant-translate.txt', 'grant.txt', 'deep-features-scene (1).pdf', 'at least this file.txt', 'students.txt', 'Small dataset face recognition.pptx'}\n",
            "Results for:  Protasov\n",
            "\t {'grant.txt', 'deep-features-scene (1).pdf', 'dsa.pdf', 'at least this file.txt', 'students.txt', 'MS2 - Problems of multithread programming.pptx', 'cs.pdf'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}