{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"Evaluation_template.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"_EpOTygSO1xM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"3e903e87-67e5-4573-818c-931c1b3b78c0","executionInfo":{"status":"ok","timestamp":1586888856282,"user_tz":-180,"elapsed":742,"user":{"displayName":"Руфина Галиева","photoUrl":"","userId":"18169663745315424110"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y-Ye5B-lCStX","colab_type":"text"},"source":["# Measure and improve\n","\n","\n","All these weeks we were exploring different ways to build a search engine: based on vectors, graphs, trees. We were adding new features such as spell-checking and suggest. This all might seem good, however, we have no ways to tell if our changes were indeed enhancements. How do we know that search engine's quality haven't deteriorated because of our changes? How do we know if cosine scoring is better than okapi scoring? How can we check if applying language models instead of vector-space models is beneficial? \n","\n","All we need is a way to ***evaluate*** our search engine. We will consider some of the popular evaluation techniques:\n","\n","1. Mean Average Precision\n","2. Normalized Discounted Cumulative Gain (NDCG)\n","3. 11-Point Interpolated Average\n","\n","We will apply them in the context of ranking with language models and will compare two ways of smoothing: additive and Jelinek-Mercer smoothing.  \n","\n","It's best to go through the [book](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf) *chapter 8*, concretely, *8.4* to understand the key concepts of this lab. Here we will only present some excerpts from it.\n"]},{"cell_type":"markdown","metadata":{"id":"c6i59IBnCStY","colab_type":"text"},"source":["## Data\n","\n","There is a number of ways to evaluate a search engine, however, all of them require so called relevance judgements - human assigned scores of relevance between query-doc_id pairs. We will use Cranfield relevance judgements collection, download it from [here](https://drive.google.com/drive/folders/1u6dSUqdrXsckSHmVHNNE27eDiajMWLl2?usp=sharing). \n","\n","This was the pioneering test collection in allowing precise quantitative measures of information retrieval effectiveness, but is nowadays too small for anything but the most elementary pilot experiments (which is exactly what we are doing here). Collected in the United Kingdom starting in the late 1950s, it contains 1400 abstracts of aerodynamics journal articles, a set of 225 queries, and exhaustive relevance judgments of all (query, document) pairs.\n","\n","![image.png](attachment:image.png)\n","\n","\n","It comes in a json format, the detailed description is available in the readme file.\n"]},{"cell_type":"code","metadata":{"id":"TsQtnmZTCStY","colab_type":"code","colab":{}},"source":["import json\n","import os\n","\n","def read_cranfield(path):\n","    # TODO: read the cranfield data\n","    # relevance should be a dictionary, query_id:[(relevant_doc_id1, score1), (relevant_doc_id2, score2), ...]\n","    with open(os.path.join(path, 'cranfield_data.json'), \"r\") as read_file:\n","      documents = json.load(read_file)\n","    with open(os.path.join(path, 'cran.qry.json'), \"r\") as read_file:\n","      queries = json.load(read_file)\n","    with open(os.path.join(path, 'cranqrel.json'), \"r\") as read_file:\n","      relevance = json.load(read_file)\n","\n","    relevance_new = {}\n","    for rel in relevance:\n","      query_id = int(rel['query_num'])\n","      position = int(rel['position'])\n","      doc_id = int(rel['id'])\n","\n","      if query_id not in relevance_new:\n","        relevance_new[query_id] = [(doc_id, position)]\n","      else:\n","        relevance_new[query_id].append((doc_id, position))\n","    \n","    return documents, queries, relevance_new\n","\n","cran_orig_path = '/content/drive/My Drive/InformationRetrieval2020/Evaluation/cranfield_data' # 'cranfield_data/'\n","documents, queries, relevance = read_cranfield(cran_orig_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DzJLaT6FCStc","colab_type":"text"},"source":["### Test "]},{"cell_type":"code","metadata":{"id":"eS3RKFL3CStc","colab_type":"code","colab":{}},"source":["assert len(documents) == 1400 \n","assert len(relevance) == 225"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RdHeqta-CStf","colab_type":"text"},"source":["## 1. Mean Average Precision\n","\n","The most standard metric among the TREC community is *Mean Average Precision* *(MAP)*, which provides a single-figure measure of quality across recall levels. Among evaluation measures, MAP has been shown to have especially good discrimination and stability. For a single information need, Average Precision is the average of the precision value obtained for the set of top k documents existing\n","after each relevant document is retrieved, and this value is then averaged over information needs (queries). That is, if the set of relevant documents for an information need q<sub>j</sub> ∈ Q is {d<sub>1</sub>, . . . d<sub>m<sub>j</sub></sub>} and R<sub>jk</sub> is the set of ranked retrieval results from the top result until you get to document d<sub>k</sub>, then\n","\n","![](https://i.imgur.com/EGQMHVq.png)\n","\n","Implement this metric in the `mean_avg_precision` function.\n"]},{"cell_type":"code","metadata":{"id":"lgSFT_zmCStf","colab_type":"code","colab":{}},"source":["def mean_avg_precision(search_results, relevance):\n","    # TODO: calculate MAP score for search results, treating relevance judgments as binary - either relevant or not.\n","    #\n","    # search_results: list of lists of ranked results for each query [[doc_id1, doc_id2,...], ...]\n","    # note that for tests to pass, the i-th result in search_results should correspond to (i+1)-th query_id.  \n","    # relevance: dict, query_id:[(relevant_doc_id1, score1), (relevant_doc_id2, score2), ...]    \n","    n_queries = len(search_results) \n","    MAP = 0.0\n","    for query_id, results in enumerate(search_results):\n","      average_precision = 0\n","      relevant_doc_ids = np.unique([doc[0] for doc in relevance[query_id+1]])\n","      n_relevant = len(relevant_doc_ids)\n","\n","      correct_queeses = 0.0\n","      for i, doc_id in enumerate(results):\n","        if doc_id in relevant_doc_ids:\n","          correct_queeses += 1\n","          average_precision += correct_queeses/(i+1)\n","\n","      average_precision=average_precision/n_relevant\n","\n","      MAP += average_precision\n","    return MAP/n_queries"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnkI-pNgCSti","colab_type":"text"},"source":["### 1.1 Test"]},{"cell_type":"code","metadata":{"id":"5xhLSAlRCStj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"0c9387ce-b44b-4b28-e7e9-fef4b486354c","executionInfo":{"status":"ok","timestamp":1586888869594,"user_tz":-180,"elapsed":746,"user":{"displayName":"Руфина Галиева","photoUrl":"","userId":"18169663745315424110"}}},"source":["import numpy as np\n","\n","test_relevance = {1: [(9, 1), (1, 2), (8, 3)], 2: [(5, 1), (9, 2), (6, 3)], \n","                  3: [(9, 1), (4, 2), (6, 3)], 4: [(10, 1), (4, 2), (7, 3)], \n","                  5: [(4, 1), (2, 2), (8, 3)], 6: [(2, 1), (9, 2), (4, 3)], \n","                  7: [(1, 1), (2, 2), (3, 3)], 8: [(3, 1), (2, 2), (6, 3)], \n","                  9: [(1, 1), (4, 2), (3, 3)], 10: [(10, 1), (7, 2), (8, 3)]}\n","test_results = [[4, 5, 3, 6, 1, 2, 8, 9, 10, 7], [7, 5, 6, 3, 1, 8, 9, 4, 2, 10], \n","                [8, 3, 4, 5, 9, 6, 1, 10, 2, 7], [4, 5, 7, 3, 6, 10, 1, 9, 2, 8], \n","                [4, 8, 3, 5, 6, 7, 2, 1, 10, 9], [9, 7, 6, 5, 2, 4, 10, 1, 3, 8], \n","                [3, 1, 5, 2, 10, 6, 7, 9, 8, 4], [9, 2, 4, 10, 8, 3, 7, 6, 1, 5], \n","                [3, 4, 6, 1, 5, 10, 7, 2, 8, 9], [8, 10, 4, 1, 3, 7, 5, 6, 9, 2]]\n","\n","\n","map_test = mean_avg_precision(test_results, test_relevance)\n","print(\"map_test\", map_test)\n","assert np.isclose(map_test, 0.646, atol=1e-03)\n","assert mean_avg_precision(test_results[:5], test_relevance) > mean_avg_precision(test_results[5:10], test_relevance)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["map_test 0.6464285714285715\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Um5SosKICStm","colab_type":"text"},"source":["## 2. Normalized Discounted Cumulative Gain\n","\n","There is also so called NDCG metric, which is designed for situations of non-binary notions of relevance. It is evaluated over some number *k* of top search results (in our case we will evaluate over the whole dataset, since it is small). \n","\n","For a set of queries Q, let *R(j,d)* be the relevance score assessors gave to document *d* for query *j*. Then,\n","\n","![](https://i.imgur.com/LLogCYa.png)\n","\n","where Z<sub>kj</sub> is a normalization factor calculated to make it so that a perfect ranking’s NDCG at *k* for query *j* is 1. In other words, we divide calculated DCG score by ideal DCG score. \n","\n","Implement this metric in `NDCG` function."]},{"cell_type":"code","metadata":{"id":"lp74ZuEXCStn","colab_type":"code","colab":{}},"source":["import math \n","\n","def DCG(query_results, query_relevance):\n","  \"\"\"\n","  computes DCG for one query, we dont take top k but consider all results as being top k\n","  param: query_relevance in format [(doc_id, rank), (9, 1), (1, 2), (8, 3)]\n","  param: query_results in format [doc_id, 4, 5, 3, 6, 1, 2, 8, 9, 10, 7], this metric take order into evaluation as well\n","  \"\"\"\n","  dcg = 0\n","  query_relevance = {doc_id:rank for doc_id, rank in query_relevance}\n","  for i, doc_id in enumerate(query_results):\n","    m=i+1\n","    try: rel = 5 - query_relevance[doc_id] # HARD CODED NUMBER, не хорошо\n","    except KeyError: rel = 0\n","    dcg += (2**rel-1)/np.log2(m+1)\n","  return dcg\n","\n","def PDCG(query_results, query_relevance):\n","  \"\"\"\n","  computes normalization factor for NDCG, e.g. best possible DCG for the quesry\n","  param: query_relevance in format [(doc_id, rank), (9, 1), (1, 2), (8, 3)]\n","  \"\"\"\n","  # make the best order in query_results and call DCG\n","  pdcg = 0\n","  for i, qr in enumerate(query_relevance):\n","    doc_id, rank = qr[0], qr[1]\n","    if doc_id not in query_results:\n","      continue\n","    m = i+1\n","    rel = 5-rank\n","    pdcg += (2**rel-1)/np.log2(m+1)\n","  if pdcg == 0:\n","    pdcg = 1\n","  return pdcg\n","\n","\n","def NDCG(search_results, relevance):\n","    # TODO: compute NDCG score for search results. Here relevance is not considered as binary - the bigger\n","    # the judgement score is, the more relevant is the document to a query. Because in our cranfield dataset relevance\n","    # judgements are presented in a different way (1 is most relevant, 4 is least), we will need to smth with it. \n","    # The simplest is to invert it, replacing each score with (5-score). For example, if the score was 2, it becomes 5-2=3.\n","    # To find normalization factor for each query, think in this direction - for this particular query what would be an\n","    # ideal DCG score? What documents should have (ideally) been returned by the search engine to maximize the DCG score?\n","    # When you find it, just normalize the real DCG score by ideal DCG score, that's it.\n","    #\n","    # search_results: list of lists of ranked results for each query [[doc_id1, doc_id2,...], ...]\n","    # note that for tests to pass, the i-th result in search_results should correspond to (i+1)-th query_id.  \n","    # relevance: dict, query_id:[(relevant_doc_id1, score1), (relevant_doc_id2, score2), ...]       \n","\n","    n_queries = len(search_results)\n","    ndcg= 0\n","    for doc_id, query_results in enumerate(search_results):\n","      query_relevance = relevance[doc_id+1]\n","      dcg = DCG(query_results, query_relevance)\n","      best_dcg = PDCG(query_results, query_relevance)\n","      ndcg += dcg/best_dcg\n","    return ndcg/n_queries"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1WVZA_tXCStp","colab_type":"text"},"source":["### 2.1 Test"]},{"cell_type":"code","metadata":{"id":"y-b9YAQgCStq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"997bacee-23b0-4f59-eeb3-6011cfb8dc87","executionInfo":{"status":"ok","timestamp":1586888875616,"user_tz":-180,"elapsed":689,"user":{"displayName":"Руфина Галиева","photoUrl":"","userId":"18169663745315424110"}}},"source":["ndcg_test = NDCG(test_results, test_relevance)\n","print(\"ndcg_test\", ndcg_test)\n","assert np.isclose(ndcg_test, 0.640, atol=1e-03)\n","assert NDCG(test_results[:5], test_relevance) < NDCG(test_results[5:10], test_relevance)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["ndcg_test 0.6409675295633275\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W6yD7dNlCStt","colab_type":"text"},"source":["## 3. 11-Point Interpolated Average (Bonus task)\n","\n","In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by the top k retrieved documents. For each such set, precision and recall values can be plotted to give a precision-recall curve, such as this one (blue line):\n","\n","![](https://i.imgur.com/QnvDLAJ.png)\n","\n","Precision-recall curves have a distinctive saw-tooth shape: if the *(k + 1)<sup>th</sup>* document retrieved is nonrelevant then recall is the same as for the top k documents, but precision has dropped. If it is relevant, then both precision and recall increase, and the curve jags up and to the right.\n","\n","It is often useful to remove these jiggles and the standard way to do this is with an *interpolated precision*: the interpolated precision *p<sub>interp</sub>* at a certain recall level *r* is defined as the highest precision found for any recall level *r′* ≥ *r*:\n","\n","![](https://i.imgur.com/GMl2rQw.png)\n","\n","The justification is that almost anyone would be prepared to look at a few more documents if it would increase the percentage of the viewed set that were relevant (that is, if the precision of the larger set is higher). Interpolated precision is shown by a red line in the figure above.\n","\n","Examining this information for a single query may be useful, but usually we are more interested in a composite metrics, which will score over all test queries. The traditional way of doing this is the *11-point interpolated average*. For each information need, the interpolated precision is measured at the 11 recall levels of 0.0, 0.1, 0.2, . . . , 1.0. Then we average interpolated precision over all queries in the test collection and plot, like here:\n","\n","![](https://i.imgur.com/6wDmtp2.png)\n","\n","\n","Plotting a number of such graphs for different versions of search engine on the same plot helps to compare their performance.\n","\n","You need to incorporate this logic into `eleven_points_interpolated_avg` function. Break it down to subfuctions as it seems necessary to you."]},{"cell_type":"code","metadata":{"id":"lCQH5WbUCStt","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def eleven_points_interpolated_avg(search_results, relevance, plot=True):\n","    # TODO: return 11-points interpolated average over all queries. Refer to chapter 8.4 for explanation.\n","    # First calculate values of precision-recall curve for each query, interpolate them, and average over all queries.\n","    # This function is intended to use when for each query all documents are scored until the last relevant element\n","    # is met. Treats relevance judgments as binary - either relevant or not.\n","    # note that for tests to pass, the i-th result in search_results should correspond to (i+1)-th query_id.\n","    #\n","    # search_results: list of lists of ranked results for each query [[doc_id1, doc_id2,...], ...]\n","    # note that for tests to pass, the i-th result in search_results should correspond to (i+1)-th query_id.  \n","    # relevance: dict, query_id:[(relevant_doc_id1, score1), (relevant_doc_id2, score2), ...]\n","    # return: interpolated_avg, list of 11 values    \n","    \n","    # Precision/Recall calculate\n","    avg_precision = np.zeros(11)\n","    interpolated_recall = [i/10 for i in range(11)]\n","\n","    n_queries = len(search_results) # NEED TO TAKE CARE THAT WE NEED TO STOP WHEN LAST RELEVANT ELEMENT IS MET\n","    for query_id, results in enumerate(search_results):\n","      precision, recall = [], []\n","      relevant_doc_ids = np.unique([doc[0] for doc in relevance[query_id+1]])\n","      n_relevant = len(relevant_doc_ids)\n","\n","      correct_queeses = 0.0\n","      for i, doc_id in enumerate(results):\n","        if doc_id in relevant_doc_ids:\n","          correct_queeses += 1\n","        precision.append( correct_queeses/(i+1) )\n","        recall.append( correct_queeses/n_relevant )\n","      # interpolate\n","      interpolated_precision = []\n","\n","      recall = np.array(recall)\n","      precision = np.array(precision)\n","\n","      for recall_level in interpolated_recall:\n","        idxs = np.where(recall>=recall_level)[0]\n","        if not len(idxs):\n","          interpolated_precision.append(0)\n","        else:\n","          interpolated_precision.append(np.max(np.take_along_axis(precision, idxs, axis=0)))\n","      avg_precision+=interpolated_precision\n","\n","    if plot:\n","      plt.plot(interpolated_recall, avg_precision)\n","      plt.show()\n","\n","    return avg_precision/n_queries, avg_precision, interpolated_recall"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdZDYzhUCStw","colab_type":"text"},"source":["### 3.1  Test"]},{"cell_type":"code","metadata":{"id":"kSA_NMDHCStw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"cb56431e-48fa-43ab-b35d-39b0c003e286","executionInfo":{"status":"ok","timestamp":1586888886906,"user_tz":-180,"elapsed":742,"user":{"displayName":"Руфина Галиева","photoUrl":"","userId":"18169663745315424110"}}},"source":["eleven_test, avg_precision, interpolated_recall = eleven_points_interpolated_avg(test_results, test_relevance, plot=True)\n","assert all(eleven_test[k] >= eleven_test[k+1] for k in range(len(eleven_test)-1))"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"JC06oO_SCSty","colab_type":"text"},"source":["## 4. Evaluation\n","\n","Now we want to apply these metrics and compare two ways of smoothing in ranking with language models. Since we only have relevance scores for Cranfield data, we will run search and do evaluations based on it.\n","\n","### LM ranking"]},{"cell_type":"code","metadata":{"id":"B_iCXnsrCStz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":103},"outputId":"b6790e08-3def-42e6-fdaa-0aed10b9206a","executionInfo":{"status":"ok","timestamp":1586888903909,"user_tz":-180,"elapsed":940,"user":{"displayName":"Руфина Галиева","photoUrl":"","userId":"18169663745315424110"}}},"source":["# TODO: copy here your lm_rank_documents function from the last lab\n","import nltk\n","import numpy as np\n","import string\n","from nltk.corpus import stopwords\n","from collections import Counter\n","from nltk import word_tokenize\n","nltk.download('stopwords')\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","all_data = []\n","for doc in documents:\n","    doc_id, doc_content = int(doc['id']), doc['body']\n","    all_data.append(doc_content)\n","\n","vectorizer = CountVectorizer(stop_words='english')\n","counts_data = vectorizer.fit_transform(all_data)\n","terms = vectorizer.get_feature_names()\n","print(\"vocabulary size\", len(terms))\n","print(\"first 100 terms: \", terms[0:100])\n","\n","\n","def lm_rank_documents(query, tdm, terms_list, smoothing='additive', param=0.001):\n","    # TODO: score each document in tdm using this document's language model\n","    # implement two types of smoothing. Looks up term frequencies in tdm\n","    # return document scores in a convenient form\n","    # param is alpha for additive / lambda for jelinek-mercer\n","    \"\"\"\n","    :param query: dict, term:count            \n","    :param tdm: term-document matrix\n","    :param terms_list: vocabulary list\n","    :param smoothing: which smoothing to apply, either 'additive' or 'jelinek-mercer'\n","    :param param: alpha for additive / lambda for jelinek-mercer\n","    :return: list of scores, list of doc_ids sorted by their scores \n","    \"\"\"\n","    n_docs = tdm.shape[0]\n","    doc_lengths = tdm.sum(axis=1)\n","    len_collection = np.sum(doc_lengths)\n","    scores = np.zeros(n_docs)\n","    for term in query.keys():\n","        # check if term exists\n","        if term in terms_list:\n","            # get term's id\n","            term_id = terms_list.index(term)\n","        else:\n","            continue\n","        query_tf = query[term]\n","        # calculate collection frequency of a term\n","        collection_tf = np.sum(tdm[:, term_id])\n","        for doc_id in range(n_docs):\n","            doc_tf = tdm[doc_id, term_id]\n","            # apply smoothing of any\n","            if smoothing == 'additive':\n","                doc_score_factor = (doc_tf + param) / (doc_lengths[doc_id] + param*len(terms_list))\n","            elif smoothing == 'jelinek':\n","                doc_score_factor = param*doc_tf/doc_lengths[doc_id] + (1-param)*collection_tf/len_collection\n","            else:\n","                doc_score_factor = doc_tf/doc_lengths[doc_id]\n","            doc_score_factor = doc_score_factor**query_tf\n","\n","            if doc_id not in scores:\n","                scores[doc_id] = 1\n","            # accumulate scores\n","            scores[doc_id] *= doc_score_factor\n","    # sort doc_ids by scores\n","    sorted_doc_ids = np.argsort(-scores)\n","    return sorted_doc_ids\n","\n","def prepare_query(raw_query):\n","    # lower-case, remove punctuation and stopwords\n","    stop_words = list(string.punctuation) + stopwords.words('english')\n","    return Counter([i for i in word_tokenize(raw_query.lower()) if i not in stop_words])\n","\n","\n","def process_query(raw_query, counts_data, terms, smoothing, param):\n","    # TODO process user query and print search results including document category, id, score, and some part of it\n","    query = prepare_query(raw_query)\n","    return lm_rank_documents(query, counts_data, terms, smoothing, param)[:10]"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","vocabulary size 7184\n","first 100 terms:  ['00', '000', '0001', '0005', '000degree', '000degreek', '000k', '001', '002', '003', '004', '00675', '008', '01', '010', '012', '013', '014', '02', '02025', '025', '028', '03', '04', '05', '06', '064', '066', '07', '08', '09', '0904', '0degree', '0degrees', '10', '100', '1000', '100degrees', '100x10', '101', '104', '1081', '109', '10degree', '10degrees', '10g', '11', '110', '1103', '1103a', '111', '113', '1135', '117', '11in', '12', '120', '1211', '124', '125', '1250', '12degree', '13', '130', '1300', '14', '140', '1400', '1400degreek', '142', '1428', '1460', '14in', '14x10', '15', '150', '1500', '153', '15degree', '15x10', '15x106', '16', '165', '17', '170', '1730', '18', '180', '180degree', '182', '1856', '1869', '1873', '1875', '18degree', '18in', '19', '190', '1909', '1910']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"59hkAkhlCSt3","colab_type":"text"},"source":["### Additive smoothing VS Jelinek-Mercer smooting"]},{"cell_type":"code","metadata":{"id":"4oJokvjRuub6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":365},"outputId":"93c1dcfa-1911-4d1e-b690-40d295ba7a77","executionInfo":{"status":"ok","timestamp":1586889265943,"user_tz":-180,"elapsed":359313,"user":{"displayName":"Руфина Галиева","photoUrl":"","userId":"18169663745315424110"}}},"source":["# TODO: compare these two types of smoothing with evaluation metrics you've just impemented. Follow the output format given. \n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","print('Evaluation results for additive smoothing:')\n","test_results, test_relevance = [], {}\n","for query in queries:\n","  query_id, query_content = query['query number'], query['query']\n","  if query_id not in relevance:\n","    continue\n","  test_relevance[query_id] = relevance[query_id]\n","  results = process_query(query_content, counts_data, terms, 'additive', 0.001)\n","  test_results.append(results)\n","\n","print(f'MAP {mean_avg_precision(test_results, test_relevance)}')\n","print(f'NDCG {NDCG(test_results, test_relevance)}')\n","eleven_test_additive, avg_precision_additive, interpolated_recall = eleven_points_interpolated_avg(test_results, test_relevance, plot=False)\n","\n","print('Evaluation results for jelinek-mercer smoothing:')\n","test_results, test_relevance = [], {}\n","for query in queries:\n","  query_id, query_content = query['query number'], query['query']\n","  if query_id not in relevance:\n","    continue\n","  test_relevance[query_id] = relevance[query_id]\n","  results = process_query(query_content, counts_data, terms, 'jelinek', 0.95)\n","  test_results.append(results)\n","\n","print(f'MAP {mean_avg_precision(test_results, test_relevance)}')\n","print(f'NDCG {NDCG(test_results, test_relevance)}')\n","eleven_test_additive, avg_precision_jm, interpolated_recall = eleven_points_interpolated_avg(test_results, test_relevance, plot=False)\n","\n","plt.plot(interpolated_recall, avg_precision_additive, label = 'additive')\n","plt.plot(interpolated_recall, avg_precision_jm, label = 'jelinek')\n","plt.legend()\n","plt.show()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Evaluation results for additive smoothing:\n","MAP 0.015598045869625961\n","NDCG 0.19786870893809566\n","Evaluation results for jelinek-mercer smoothing:\n","MAP 0.014983594242991097\n","NDCG 0.20016121648359106\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}