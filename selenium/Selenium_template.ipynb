{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse me if you can #\n",
    "\n",
    "Sometimes when crawling we have to parse websites that turn out to be SaaS - i.e., there is a special JS application which renders documents and which is downloaded first. Therefore, data that is to be rendered initially comes in a proprietary format. One of the examples is Google Drive. Last time we downladed and parsed some files from GDrive, however, we didn't parse GDrive-specific file formats, such as google sheets or google slides.\n",
    "\n",
    "Today we will learn to obtain and parse such data using Selenium - a special framework for testing web-applications.\n",
    "\n",
    "## 1. Getting started\n",
    "\n",
    "Let's try to load and parse the page the way we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не удалось открыть файл, поскольку в вашем браузере отключено использование JavaScript. Включите его и перезагрузите страницу.Некоторые функции PowerPoint не поддерживаются в Google Презентациях. Они будут удалены, если вы измените документ.Подробнее…6. Approximate nearest neighbours search 2. Trees   Смотреть  Открыть доступВойтиИспользуемая вами версия браузера больше не поддерживается. Установите поддерживаемую версию браузера.Закрытьdocument.getElementById('docs-unsupported-browser-bar').addEventListener('click', function () {this.parentNode.parentNode.removeChild(this.parentNode);return false;});ФайлИзменитьВидСправкаСпециальные возможностиОтладкаНесохраненные изменения: ДискПоследние изменения      Специальные возможности  Только просмотр     DOCS_timing['che'] = new Date().getTime();DOCS_timing['chv'] = new Date().getTime();Презентация в виде HTML(function(){/*\n",
      "\n",
      " Copyright The Closure Library Authors.\n",
      " SPDX-License-Identifier: Apache-2.0\n",
      "*/\n",
      "var a=this||self;function b(){this.a=t\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "resp = requests.get(\"https://docs.google.com/presentation/d/1LuZvz3axBD8UuHLagdv0EbhsGEWJmpd7gN5KjwYCp9Y/edit?usp=sharing\")\n",
    "soup = BeautifulSoup(resp.text, 'lxml')\n",
    "print(soup.body.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the output is not what we expect. So, what can we do when a page is not being loaded right away, but is rather rendered by a script? Browser engines can help us get data. Let's try to load the same web-page, but do it in a different way: let's give some time to a browser to load the scripts and run them; and then will work with DOM (Document Object Model), but will get it from browser engine itself, not from BeautifulSoup.\n",
    "\n",
    "Where do we get browser engine from? Simply installing a browser will do the thing. How do we send commands to it from code and retrieve DOM? Service applications called drivers will interpret out commands and translate them into browser actions.\n",
    "\n",
    "\n",
    "For each browser engine suport you will need to:\n",
    "1. install browser itself;\n",
    "2. download 'driver' - binary executable, which passed commands from selenium to browser. E.g. [Gecko == Firefox](https://github.com/mozilla/geckodriver/releases), [ChromeDriver](http://chromedriver.storage.googleapis.com/index.html);\n",
    "3. unpack driver into a folder under PATH environment variable. Or specify exact binary location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download driver\n",
    "\n",
    "And place it in any folder or under PATH env. variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch browser\n",
    "\n",
    "This will open browser window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(r'C:\\Users\\Rufina\\Downloads\\selenium\\chromedriver.exe')\n",
    "# or explicitly\n",
    "# browser = webdriver.Firefox(\n",
    "#     executable_path='C:/bin/geckodriver.exe', \n",
    "#     firefox_binary='C:/Program Files/Mozilla Firefox/firefox.exe'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements found: 95\n",
      "What if just a silly approach: Forests\n",
      "of\n",
      "search\n",
      "trees\n",
      "What if a smart approach: Forests of search trees\n"
     ]
    }
   ],
   "source": [
    "# navigate to page\n",
    "browser.get('http://tiny.cc/00dhkz')\n",
    "browser.implicitly_wait(5)  # wait 5 seconds\n",
    "# browser.quit()\n",
    "# select all text parts from document\n",
    "elements = browser.find_elements_by_css_selector(\"g.sketchy-text-content-text\")\n",
    "# note that if number differs from launch to launch this means better extend wait time\n",
    "print(\"Elements found:\", len(elements))\n",
    "\n",
    "# oh no! It glues all the words!\n",
    "print(\"What if just a silly approach:\", elements[0].text)\n",
    "\n",
    "# GDrive stores all text blocks word-by-word\n",
    "subnodes = elements[0].find_elements_by_css_selector(\"text\")\n",
    "text = \" \".join(n.text for n in subnodes)\n",
    "print(\"What if a smart approach:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "- Too slow, wait for browser to open, browser to render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headless mode\n",
    "Browsers (at least [FF](https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Headless_mode), [Chrome](https://intoli.com/blog/running-selenium-with-headless-chrome/), IE) have headless mode - no window rendering and so on. Means it should work much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions() # webdriver.FirefoxOptions()\n",
    "\n",
    "# options.add_argument('headless')\n",
    "options.add_argument('window-size=1200x600')\n",
    "browser = webdriver.Chrome(options=options) # webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements found: 95\n",
      "What if just a silly approach: Forests\n",
      "of\n",
      "search\n",
      "trees\n",
      "What if a smart approach: Forests of search trees\n"
     ]
    }
   ],
   "source": [
    "## SAME CODE\n",
    "\n",
    "# navigate to page\n",
    "browser.get('http://tiny.cc/00dhkz')\n",
    "browser.implicitly_wait(5)  # wait 5 seconds\n",
    "\n",
    "# select all text parts from document\n",
    "elements = browser.find_elements_by_css_selector(\"g.sketchy-text-content-text\")\n",
    "# note that if number differs from launch to launch this means better extend wait time\n",
    "print(\"Elements found:\", len(elements))\n",
    "\n",
    "# oh no! It adds NEW LINE. Behavior differs!!!!\n",
    "print(\"What if just a silly approach:\", elements[0].text)\n",
    "\n",
    "# GDrive stores all text blocks word-by-word\n",
    "subnodes = elements[0].find_elements_by_css_selector(\"text\")\n",
    "text = \" \".join(n.text for n in subnodes)\n",
    "print(\"What if a smart approach:\", text)\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB \n",
    "Note, that browser behavior differs for the same code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Task \n",
    "Our lectures usually have lot's of links. Here are the links to oviginal versions of the documents.\n",
    "\n",
    "[4. Vector space](https://docs.google.com/presentation/d/1UxjGZPPrPTM_3lCa_gWTk8yZI_qNmTKwtMxr8JZQCIc/edit?usp=sharing)\n",
    "\n",
    "[6. search trees](https://docs.google.com/presentation/d/1LuZvz3axBD8UuHLagdv0EbhsGEWJmpd7gN5KjwYCp9Y/edit?usp=sharing)\n",
    "\n",
    "[7-8. Web basics](https://docs.google.com/presentation/d/1bgsCgpjMcQmrFpblRI6oH9SnG4bjyo5SzSSdKxxHNlg/edit?usp=sharing)\n",
    "\n",
    "Please complete the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Inverted index for slides with numbers\n",
    "I want to type a word, and it should say which slides of which lecture has this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and parsing text and images from google slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def getTextAndImgsFromSlides(url = 'http://tiny.cc/00dhkz'):    \n",
    "    slides_text = dict() # dictionary slide_num : slide_text\n",
    "    img_list = [] # list of image urls \n",
    "    #TODO: parse google slides and save all text and image urls in slides_text and img_list\n",
    "    # you should get the contents from ALL slides - however, you will see that at one moment \n",
    "    # of time only single slide + few slide previews on the left are visible. To be able to    \n",
    "    # reach all slides you will need to scroll to and click these previews. While slide contents \n",
    "    # can be obtained from previews themselves, speaker notes (which you also have to extract)\n",
    "    # can be viewed only if a particular slide is open.\n",
    "    # to scroll the element of interest into view, use can this: \n",
    "    # browser.execute_script(\"arguments[0].scrollIntoView();\", el)\n",
    "    # to click the element, use can use ActionChains library   \n",
    "\n",
    "    options = webdriver.ChromeOptions() \n",
    "    options.add_argument('headless')\n",
    "    options.add_argument('window-size=1200x600')\n",
    "    browser = webdriver.Chrome(options=options) \n",
    "\n",
    "    browser.get(url)\n",
    "    browser.implicitly_wait(10)  # wait 5 seconds\n",
    "    actions = ActionChains(browser)\n",
    "    elems_num = 0\n",
    "    i = 1\n",
    "    # get left bar that is visible for now marking slides that are visited\n",
    "    # click on all of them and then scroll down, update left bar\n",
    "    \n",
    "    left_bar = browser.find_elements_by_css_selector('g.punch-filmstrip-thumbnail')\n",
    "    while True:\n",
    "        all_visited = True\n",
    "        for lb in left_bar:\n",
    "            # get slide number\n",
    "            slide_n = lb.text.split()\n",
    "            if len(slide_n)==0:\n",
    "                continue\n",
    "            slide_n = slide_n[0]\n",
    "            if slide_n not in slides_text:\n",
    "                all_visited = False\n",
    "                # click on that slide\n",
    "                lb.click()\n",
    "                time.sleep(0.1)\n",
    "                # take info\n",
    "                text = ''\n",
    "                images = browser.find_elements_by_tag_name('image')\n",
    "                slide = browser.find_element_by_xpath(\"//div[@id='workspace-container']\")\n",
    "                elements = slide.find_elements_by_css_selector('g.sketchy-text-content-text')\n",
    "                speaker_notes = browser.find_element_by_xpath(\"//div[@id='speakernotes-workspace']\")\n",
    "                speaker_notes = speaker_notes.find_elements_by_css_selector('g.sketchy-text-content-text')\n",
    "                for e in elements:\n",
    "                    text += e.text\n",
    "                    text += ' '\n",
    "                for e in speaker_notes:\n",
    "                    text += e.text\n",
    "                    text += ' '\n",
    "                text = text.replace('\\n', ' ')\n",
    "                text = re.sub(' +', ' ', text)\n",
    "                slides_text[slide_n] = text\n",
    "        # now we check if at least one slide was new for now, if there was updates we scroll down\n",
    "        if all_visited:\n",
    "            break\n",
    "        left_bar = browser.find_elements_by_css_selector('g.punch-filmstrip-thumbnail')\n",
    "    \n",
    "    for img in images:\n",
    "        img_list.append(img.get_attribute('href'))\n",
    "\n",
    "    browser.quit()\n",
    "    return slides_text, img_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing three presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = [\"https://docs.google.com/presentation/d/1UxjGZPPrPTM_3lCa_gWTk8yZI_qNmTKwtMxr8JZQCIc/edit?usp=sharing\", \n",
    "         \"https://docs.google.com/presentation/d/1LuZvz3axBD8UuHLagdv0EbhsGEWJmpd7gN5KjwYCp9Y/edit?usp=sharing\",\n",
    "         \"https://docs.google.com/presentation/d/1bgsCgpjMcQmrFpblRI6oH9SnG4bjyo5SzSSdKxxHNlg/edit?usp=sharing\"]\n",
    "\n",
    "all_imgs = []\n",
    "all_texts = dict()\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    texts, imgs = getTextAndImgsFromSlides(link)\n",
    "    all_imgs += imgs\n",
    "    for slide_n in texts:\n",
    "        all_texts[f'{i}_{slide_n}'] = texts[slide_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "texts, imgs = getTextAndImgsFromSlides('http://tiny.cc/00dhkz')\n",
    "\n",
    "assert len(texts) == 35 # equal to the total number of slides in the presentation \n",
    "print(len(texts))\n",
    "\n",
    "assert len(imgs) > 26 # can be more than that due to visitor icons\n",
    "print(len(imgs))\n",
    "\n",
    "assert any(\"Navigable\" in value for value in texts.values()) # word is on a slide\n",
    "assert any(\"MINUS\" in value for value in texts.values()) # word is in speaker notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an inverted index and search using it (boolean retrieval is just ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rufina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#TODO: build an inverted index and enable search in it\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = nltk.corpus.stopwords.words('english')\n",
    "        self.ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "\n",
    "    # word tokenize text using nltk lib\n",
    "    def tokenize(self, text):\n",
    "        return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "    # stem word using provided stemmer\n",
    "    def stem(self, word, stemmer):\n",
    "        return stemmer.stem(word)\n",
    "\n",
    "\n",
    "    # check if word is appropriate - not a stop word and isalpha, \n",
    "    # i.e consists of letters, not punctuation, numbers, dates\n",
    "    def is_apt_word(self, word):\n",
    "        return word not in self.stop_words and word.isalpha()\n",
    "\n",
    "\n",
    "    # combines all previous methods together\n",
    "    # tokenizes lowercased text and stems it, ignoring not appropriate words\n",
    "    def preprocess(self, text):\n",
    "        tokenized = self.tokenize(text.lower())\n",
    "        return [self.stem(w, self.ps) for w in tokenized if self.is_apt_word(w)]\n",
    "\n",
    "def build_inverted_index(files_data):\n",
    "  index = dict()\n",
    "  # doc_names = dict()\n",
    "  def index_doc(doc_content, doc_id):\n",
    "    prep = Preprocessor()\n",
    "    doc_content = prep.preprocess(doc_content)\n",
    "    article_index = Counter(doc_content)\n",
    "    for term in article_index.keys():\n",
    "        article_freq = article_index[term]\n",
    "        if term not in index:                \n",
    "            index[term] = [article_freq, (doc_id, article_freq)]\n",
    "        else:\n",
    "            index[term][0] += article_freq\n",
    "            index[term].append((doc_id, article_freq))\n",
    "  #TODO build search index from files\n",
    "  for doc_id, file_name in enumerate(files_data):\n",
    "    # doc_names[doc_id] = file_name\n",
    "    index_doc(files_data[file_name], file_name)\n",
    "  return index\n",
    "\n",
    "class QueryProcessing:\n",
    "    @staticmethod\n",
    "    def prepare_query(raw_query):\n",
    "        prep = Preprocessor()\n",
    "        # pre-process query the same way as documents\n",
    "        query = prep.preprocess(raw_query)\n",
    "        # count frequency\n",
    "        return Counter(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def boolean_retrieval(query, index):\n",
    "        postings = []\n",
    "        for term in query.keys():\n",
    "            if term not in index:  # ignoring absent terms\n",
    "                continue\n",
    "            posting = index[term][1:]\n",
    "            # extract document info only\n",
    "            posting = [i[0] for i in posting]\n",
    "            postings.append(posting)\n",
    "        docs = set.intersection(*map(set,postings))\n",
    "        \n",
    "        return docs \n",
    "\n",
    "def find(query, index):\n",
    "    #TODO implement search procedure\n",
    "    # preprocess query\n",
    "    query = QueryProcessing.prepare_query(query)\n",
    "    return QueryProcessing.boolean_retrieval(query, index)\n",
    "\n",
    "\n",
    "inverted_index = build_inverted_index(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for:  architecture\n",
      "\t {'2_53', '0_16', '0_5', '0_19'}\n",
      "Results for:  algorithm\n",
      "\t {'1_13', '1_32', '1_14', '1_34', '2_22'}\n",
      "Results for:  function\n",
      "\t {'1_11', '2_34', '1_10'}\n",
      "Results for:  dataset\n",
      "\t {'0_9', '1_27', '1_23'}\n",
      "Results for:  Protasov\n",
      "\t {'0_1', '1_1', '2_1'}\n",
      "Results for:  cosine\n",
      "\t {'0_16', '1_1'}\n",
      "Results for:  модель\n",
      "\t {'0_16', '2_51'}\n",
      "Results for:  например\n",
      "\t {'0_4', '2_38', '2_53', '2_20', '2_13'}\n"
     ]
    }
   ],
   "source": [
    "queries = [\"architecture\", \"algorithm\", \"function\", \"dataset\", \n",
    "           \"Protasov\", \"cosine\", \"модель\", \"например\"]\n",
    "\n",
    "for query in queries:\n",
    "    r = find(query, inverted_index)\n",
    "    print(\"Results for: \", query)\n",
    "    print(\"\\t\", r)\n",
    "    assert len(r) > 0, \"Query should return at least 1 document\"\n",
    "    assert len(r) > 1, \"Query should return at least 2 documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Images saved\n",
    "Save all images used in a presentation as separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: load and save all images from slides on disk (from one | all presentations, both are fine)\n",
    "import urllib.request\n",
    "\n",
    "for idx, img_url in enumerate(all_imgs):\n",
    "    try: urllib.request.urlretrieve(img_url, f'{idx}.jpg')\n",
    "    except ValueError:\n",
    "        continue # its a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bonus task.  Links exploration\n",
    "\n",
    "Find all external links in these presentations and index them for search too. I.e., parse them and extend inverted index you built with external links content: when searching by word we should get not only slides that contain it, but also any links that were mentioned in slides and contain this query word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html2text\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
      "Installing collected packages: html2text\n",
      "Successfully installed html2text-2020.1.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebElement' object has no attribute 'w3c'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-c0677d15138b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mslides_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetTextAndImgsAndLinksFromSlides\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-c0677d15138b>\u001b[0m in \u001b[0;36mgetTextAndImgsAndLinksFromSlides\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[1;31m# click on a text to obtain the link\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#                     e.click()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                     \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActionChains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m                     \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                     \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslide\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_css_selector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div.docs-bubble docs-linkbubble-bubble docs-linkbubble-link-preview'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\action_chains.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw3c\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw3c_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActionBuilder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebElement' object has no attribute 'w3c'"
     ]
    }
   ],
   "source": [
    "# #TODO: bonus task\n",
    "# import requests\n",
    "# from urllib.parse import quote\n",
    "# from bs4 import BeautifulSoup\n",
    "# from bs4.element import Comment\n",
    "# import urllib.parse\n",
    "# import os\n",
    "# import html2text\n",
    "\n",
    "# def get_url_content(url):\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code == 200:\n",
    "#         h = html2text.HTML2Text()\n",
    "#         return h.handle(response.content.decode('utf-8'))\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # print(get_url_content('https://research.fb.com/blog/2014/09/fast-randomized-svd/'))\n",
    "\n",
    "# def getTextAndImgsAndLinksFromSlides(url = 'http://tiny.cc/00dhkz'):    \n",
    "#     slides_text = dict() # dictionary slide_num : slide_text\n",
    "#     img_list = [] # list of image urls \n",
    "#     links_list = []\n",
    "\n",
    "#     options = webdriver.ChromeOptions() \n",
    "#     options.add_argument('headless')\n",
    "#     options.add_argument('window-size=1200x600')\n",
    "#     browser = webdriver.Chrome(options=options) \n",
    "\n",
    "#     browser.get(url)\n",
    "#     browser.implicitly_wait(10)  # wait 5 seconds\n",
    "#     actions = ActionChains(browser)\n",
    "#     elems_num = 0\n",
    "#     i = 1\n",
    "#     # get left bar that is visible for now marking slides that are visited\n",
    "#     # click on all of them and then scroll down, update left bar\n",
    "    \n",
    "#     left_bar = browser.find_elements_by_css_selector('g.punch-filmstrip-thumbnail')\n",
    "#     while True:\n",
    "#         all_visited = True\n",
    "#         for lb in left_bar:\n",
    "#             # get slide number\n",
    "#             slide_n = lb.text.split()\n",
    "#             if len(slide_n)==0:\n",
    "#                 continue\n",
    "#             slide_n = slide_n[0]\n",
    "#             if slide_n not in slides_text:\n",
    "#                 all_visited = False\n",
    "#                 # click on that slide\n",
    "#                 lb.click()\n",
    "#                 # take info\n",
    "#                 text = ''\n",
    "#                 images = browser.find_elements_by_tag_name('image')\n",
    "#                 slide = browser.find_element_by_xpath(\"//div[@id='workspace-container']\")\n",
    "#                 elements = slide.find_elements_by_css_selector('g.sketchy-text-content-text')\n",
    "#                 speaker_notes = browser.find_element_by_xpath(\"//div[@id='speakernotes-workspace']\")\n",
    "#                 speaker_notes = speaker_notes.find_elements_by_css_selector('g.sketchy-text-content-text')\n",
    "#                 for e in elements:\n",
    "#                     text += e.text\n",
    "#                     text += ' '\n",
    "#                     # click on a text to obtain the link\n",
    "#                     e.click()\n",
    "#                     link = slide.find_element_by_css_selector('div.docs-bubble docs-linkbubble-bubble docs-linkbubble-link-preview')\n",
    "#                     link = slide.find_element_by_tag_name('a')\n",
    "#                     links_list.append(link.get_attribute('href'))\n",
    "#                 for e in speaker_notes:\n",
    "#                     text += e.text\n",
    "#                     text += ' '\n",
    "#                 text = text.replace('\\n', ' ')\n",
    "#                 text = re.sub(' +', ' ', text)\n",
    "#                 slides_text[slide_n] = text\n",
    "#         # now we check if at least one slide was new for now, if there was updates we scroll down\n",
    "#         if all_visited:\n",
    "#             break\n",
    "#         left_bar = browser.find_elements_by_css_selector('g.punch-filmstrip-thumbnail')\n",
    "    \n",
    "#     for img in images:\n",
    "#         img_list.append(img.get_attribute('href'))\n",
    "#     for link in links_list:\n",
    "#         print(link)\n",
    "#     print(len(links_list))\n",
    "\n",
    "#     browser.quit()\n",
    "#     return slides_text, img_list\n",
    "\n",
    "# out = getTextAndImgsAndLinksFromSlides()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
